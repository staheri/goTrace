diff -Naur /usr/local/go/src/internal/trace/parser.go /usr/local/go-new/go/src/internal/trace/parser.go
--- /usr/local/go/src/internal/trace/parser.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/internal/trace/parser.go	2020-12-08 14:42:36.494860992 -0700
@@ -1058,7 +1058,16 @@
 	EvUserTaskEnd       = 46 // end of task [timestamp, internal task id, stack]
 	EvUserRegion        = 47 // trace.WithRegion [timestamp, internal task id, mode(0:start, 1:end), stack, name string]
 	EvUserLog           = 48 // trace.Log [timestamp, internal id, key string id, stack, value string]
-	EvCount             = 49
+	EvChSend            = 49 // goTrace: chan send [timestamp, stack, channel id, ch_event id, value, pos]
+	EvChRecv            = 50 // goTrace: chan recv [timestamp, stack, channel id, ch_event id, value, pos]
+	EvChOp              = 51 // goTrace: chan make/close [timestamp, stack, channel id, value{0:close,1:make}]
+	EvWgAdd             = 52 // goTrace: wg add (and inited) [timestamp, stack, wg id, value]
+	EvWgWait            = 53 // goTrace: wg wait [timestamp, stack, wg id, pos]
+	EvMuLock            = 54 // goTrace: mu lock [timestamp, stack, mu id, pos]
+	EvMuUnlock          = 55 // goTrace: mu unlock [timestamp, stack, mu id]
+	EvSelect            = 56 // goTrace: select [timestamp, stack, pos]
+	EvSched             = 57 // goTrace: sched [timestamp, stack, pos, curg, aux]
+	EvCount             = 58
 )
 
 var EventDescriptions = [EvCount]struct {
@@ -1117,4 +1126,13 @@
 	EvUserTaskEnd:       {"UserTaskEnd", 1011, true, []string{"taskid"}, nil},
 	EvUserRegion:        {"UserRegion", 1011, true, []string{"taskid", "mode", "typeid"}, []string{"name"}},
 	EvUserLog:           {"UserLog", 1011, true, []string{"id", "keyid"}, []string{"category", "message"}},
+	EvChSend:            {"ChSend", 1011, true, []string{"cid","chid","val","pos"},nil}, // goTrace: chan send [timestamp, stack, channel id, ch_event id, value, pos]
+	EvChRecv:            {"ChRecv", 1011, true, []string{"cid","chid","val","pos"},nil}, // goTrace: chan send [timestamp, stack, channel id, ch_event id, value, pos]
+	EvChOp:              {"ChOp", 1011, true, []string{"cid","val"},nil},// goTrace: chan make/close [timestamp, stack, channel id, value{0:close,1:make}]
+	EvWgAdd:             {"WgAdd", 1011, true, []string{"wid","val"},nil}, // goTrace: wg add (and inited) [timestamp, stack, wg id, value]
+	EvWgWait:            {"WgWait", 1011, true, []string{"wid","pos"},nil}, // goTrace: wg wait [timestamp, stack, wg id]
+	EvMuLock:            {"MuLock", 1011, true, []string{"muid","pos"},nil},// goTrace: mu lock [timestamp, stack, mu id]
+	EvMuUnlock:          {"MuUnlock", 1011, true, []string{"muid"},nil},// goTrace: mu unlock [timestamp, stack, mu id]
+	EvSelect:            {"Select", 1011, true, []string{"pos"},nil},// goTrace: select [timestamp, stack, pos]
+	EvSched:             {"Sched", 1011, true, []string{"pos","curg","aux"},nil}, // goTrace: sched [timestamp, stack, pos, curg, aux]
 }
diff -Naur /usr/local/go/src/runtime/chan.go /usr/local/go-new/go/src/runtime/chan.go
--- /usr/local/go/src/runtime/chan.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/runtime/chan.go	2020-12-08 15:01:10.205324798 -0700
@@ -30,6 +30,7 @@
 )
 
 type hchan struct {
+	id       uint64         // goTrace: channel id 
 	qcount   uint           // total data in the queue
 	dataqsiz uint           // size of the circular queue
 	buf      unsafe.Pointer // points to an array of dataqsiz elements
@@ -55,6 +56,12 @@
 	last  *sudog
 }
 
+// goTrace
+var ( 
+	chID uint64 = 1 // goTrace
+	evID uint64 = 1 // goTrace
+)
+
 //go:linkname reflect_makechan reflect.makechan
 func reflect_makechan(t *chantype, size int) *hchan {
 	return makechan(t, size)
@@ -111,6 +118,11 @@
 	c.dataqsiz = uint(size)
 	lockInit(&c.lock, lockRankHchan)
 
+	// goTrace
+	chID = atomic.Xadd64(&chID,1) //goTrace: increment channel id
+	c.id = chID                   //goTrace: assign
+	traceChOp(c.id,1)             //goTrace: ChOp. val=1 --> make channel
+
 	if debugChan {
 		print("makechan: chan=", c, "; elemsize=", elem.size, "; dataqsiz=", size, "\n")
 	}
@@ -205,6 +217,7 @@
 	}
 
 	if sg := c.recvq.dequeue(); sg != nil {
+		sg.cid = c.id   // goTrace: set sg.cid
 		// Found a waiting receiver. We pass the value we want to send
 		// directly to the receiver, bypassing the channel buffer (if any).
 		send(c, sg, ep, func() { unlock(&c.lock) }, 3)
@@ -218,6 +231,10 @@
 			raceacquire(qp)
 			racerelease(qp)
 		}
+
+		evID = atomic.Xadd64(&evID,1)            //goTrace: increment event id
+		traceChSend(c.id, evID, elem2int(ep),1)  //goTrace: trace send event, pos:1 --> non-blocking, buffer is vacant
+
 		typedmemmove(c.elemtype, qp, ep)
 		c.sendx++
 		if c.sendx == c.dataqsiz {
@@ -247,15 +264,25 @@
 	mysg.g = gp
 	mysg.isSelect = false
 	mysg.c = c
+	// goTrace
+	mysg.cid = c.id                                  //goTrace
+	evID = atomic.Xadd64(&evID,1)                    //goTrace
+	mysg.eventid = atomic.Load64(&evID)              //goTrace
+	mysg.value = elem2int(ep)                        //goTrace 
+	traceChSend(c.id, mysg.eventid, mysg.value, 0)  //goTrace: trace send event. pos=0 --> blocked
+
 	gp.waiting = mysg
 	gp.param = nil
 	c.sendq.enqueue(mysg)
+
+
 	// Signal to anyone trying to shrink our stack that we're about
 	// to park on a channel. The window between when this G's status
 	// changes and when we set gp.activeStackChans is not safe for
 	// stack shrinking.
 	atomic.Store8(&gp.parkingOnChan, 1)
 	gopark(chanparkcommit, unsafe.Pointer(&c.lock), waitReasonChanSend, traceEvGoBlockSend, 2)
+	traceChSend(c.id, mysg.eventid, mysg.value, 2)  //goTrace: trace send event. pos=2 --> blockin send (unblocked by an arriving recver)
 	// Ensure the value being sent is kept alive until the
 	// receiver copies it out. The sudog has a pointer to the
 	// stack object, but sudogs aren't considered as roots of the
@@ -309,6 +336,13 @@
 			c.sendx = c.recvx // c.sendx = (c.sendx+1) % c.dataqsiz
 		}
 	}
+
+	evID = atomic.Xadd64(&evID, 1)           //goTrace: trace send event
+	sg.eventid = atomic.Load64(&evID)        //goTrace: trace send event
+	sg.cid=c.id                              //goTrace: trace send event
+	sg.value=elem2int(ep)                    //goTrace: trace send event
+	traceChSend(c.id, evID, elem2int(ep),3)  //goTrace: trace send event. pos=3 --> non-blocking (recv ready)
+
 	if sg.elem != nil {
 		sendDirect(c.elemtype, sg, ep)
 		sg.elem = nil
@@ -372,6 +406,7 @@
 	}
 
 	c.closed = 1
+	traceChOp(c.id,0) // goTrace: ChOp. val=0 --> close channel
 
 	var glist gList
 
@@ -511,6 +546,7 @@
 		if raceenabled {
 			raceacquire(c.raceaddr())
 		}
+		traceChRecv(c.id,0,0,1) // goTrace: trace recv event. pos=1 --> recv on closed (ch_eid:0 --> no matching send)
 		unlock(&c.lock)
 		if ep != nil {
 			typedmemclr(c.elemtype, ep)
@@ -523,6 +559,7 @@
 		// directly from sender. Otherwise, receive from head of queue
 		// and add sender's value to the tail of the queue (both map to
 		// the same buffer slot because the queue is full).
+		traceChRecv(c.id, sg.eventid , sg.value,4) // goTrace: trace recv event. pos=4 --> non-blocking recv (directly from waiting sender(unbuf) or from sender's buffer that is blocked on full queue)
 		recv(c, sg, ep, func() { unlock(&c.lock) }, 3)
 		return true, true
 	}
@@ -543,6 +580,7 @@
 			c.recvx = 0
 		}
 		c.qcount--
+		traceChRecv(c.id,0,0,2) // goTrace: trace recv event. pos=2 --> buffered channel directly from queue (ch_eid:0 & val=0 --> no matching send)
 		unlock(&c.lock)
 		return true, true
 	}
@@ -569,6 +607,7 @@
 	mysg.c = c
 	gp.param = nil
 	c.recvq.enqueue(mysg)
+	traceChRecv(c.id,0,0,0) // goTrace: trace recv event. pos=0 --> blocked recv (ch_eid=0 & val=0 --> no matching send)
 	// Signal to anyone trying to shrink our stack that we're about
 	// to park on a channel. The window between when this G's status
 	// changes and when we set gp.activeStackChans is not safe for
@@ -577,6 +616,7 @@
 	gopark(chanparkcommit, unsafe.Pointer(&c.lock), waitReasonChanReceive, traceEvGoBlockRecv, 2)
 
 	// someone woke us up
+	traceChRecv(c.id,mysg.eventid,mysg.value,3) // goTrace: trace recv event. pos=3 --> blocking recv (unblocked by an arriving sender)
 	if mysg != gp.waiting {
 		throw("G waiting list is corrupted")
 	}
@@ -834,3 +874,11 @@
 	racereleaseg(sg.g, chanbuf(c, 0))
 	raceacquire(chanbuf(c, 0))
 }
+
+//goTrace: convert element (pointer) to int
+func elem2int(elem unsafe.Pointer) uint64{
+	if elem == nil{
+		return 0
+	}
+	return uint64(*((*int)(elem)))
+}
diff -Naur /usr/local/go/src/runtime/proc.go /usr/local/go-new/go/src/runtime/proc.go
--- /usr/local/go/src/runtime/proc.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/runtime/proc.go	2020-12-08 15:01:59.477628934 -0700
@@ -313,6 +313,9 @@
 }
 
 func goready(gp *g, traceskip int) {
+	if trace.enabled{
+		traceSched(1,uint64(gp.goid),0) // goTrace: sched event. pos=1 --> goReady, aux:N/A
+	}
 	systemstack(func() {
 		ready(gp, traceskip, true)
 	})
@@ -2607,6 +2610,7 @@
 // One round of scheduler: find a runnable goroutine and execute it.
 // Never returns.
 func schedule() {
+	var aux uint64;
 	_g_ := getg()
 
 	if _g_.m.locks != 0 {
@@ -2615,6 +2619,9 @@
 
 	if _g_.m.lockedg != 0 {
 		stoplockedm()
+		if trace.enabled{
+			traceSched(2, uint64(_g_.goid),0) // goTrace: sched event. pos=2 --> schedule_g.m.lockedg != 0, aux: N/A
+		}
 		execute(_g_.m.lockedg.ptr(), false) // Never returns.
 	}
 
@@ -2658,11 +2665,13 @@
 			casgstatus(gp, _Gwaiting, _Grunnable)
 			traceGoUnpark(gp, 0)
 			tryWakeP = true
+			aux = 101 // goTrace: set sched aux. aux=101 --> schedule_goUnpark_traceReader
 		}
 	}
 	if gp == nil && gcBlackenEnabled != 0 {
 		gp = gcController.findRunnableGCWorker(_g_.m.p.ptr())
 		tryWakeP = tryWakeP || gp != nil
+		aux = 102 // goTrace: set sched aux. aux=102 --> findRunnableGCWorker
 	}
 	if gp == nil {
 		// Check the global runnable queue once in a while to ensure fairness.
@@ -2671,15 +2680,18 @@
 		if _g_.m.p.ptr().schedtick%61 == 0 && sched.runqsize > 0 {
 			lock(&sched.lock)
 			gp = globrunqget(_g_.m.p.ptr(), 1)
+			aux = 103 // goTrace: set sched aux. aux=103 --> fairness global runq 
 			unlock(&sched.lock)
 		}
 	}
 	if gp == nil {
 		gp, inheritTime = runqget(_g_.m.p.ptr())
+		aux = 104 // goTrace: set sched aux. aux=104 --> runqget
 		// We can see gp != nil here even if the M is spinning,
 		// if checkTimers added a local goroutine via goready.
 	}
 	if gp == nil {
+		aux = 105 // goTrace: set sched aux. aux=105 --> findRunnable (blocked)
 		gp, inheritTime = findrunnable() // blocks until work is available
 	}
 
@@ -2718,6 +2730,9 @@
 		startlockedm(gp)
 		goto top
 	}
+	if trace.enabled{
+		traceSched(3,uint64(gp.goid),aux) // goTrace: sched event. pos=3 --> schedule with g obtained from {aux} 
+	}
 
 	execute(gp, inheritTime)
 }
@@ -5270,6 +5285,9 @@
 			break
 		}
 		if _p_.runnext.cas(next, 0) {
+			if trace.enabled{
+				traceSched(4,uint64(gp.goid),201) // goTrace: sched event. pos=4 --> runqget, aux=201 --> return next
+			}
 			return next.ptr(), true
 		}
 	}
@@ -5278,10 +5296,16 @@
 		h := atomic.LoadAcq(&_p_.runqhead) // load-acquire, synchronize with other consumers
 		t := _p_.runqtail
 		if t == h {
+			if trace.enabled{
+				traceSched(4,uint64(gp.goid),202) // goTrace: sched event. pos=4 --> runqget, aux=202 --> return nil as q is empty
+			}
 			return nil, false
 		}
 		gp := _p_.runq[h%uint32(len(_p_.runq))].ptr()
 		if atomic.CasRel(&_p_.runqhead, h, h+1) { // cas-release, commits consume
+			if trace.enabled{
+				traceSched(4,uint64(gp.goid),203) // goTrace: sched event. pos=4 --> runqget, aux=203 --> return g from head of q
+			}
 			return gp, false
 		}
 	}
diff -Naur /usr/local/go/src/runtime/runtime2.go /usr/local/go-new/go/src/runtime/runtime2.go
--- /usr/local/go/src/runtime/runtime2.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/runtime/runtime2.go	2020-12-08 12:02:39.872231877 -0700
@@ -370,6 +370,10 @@
 	waitlink *sudog // g.waiting list or semaRoot
 	waittail *sudog // semaRoot
 	c        *hchan // channel
+
+	eventid     uint64 // goTrace: used for correlating send/recv
+	value       uint64 // goTrace: used for representing value to tracer
+	cid         uint64 // goTrace: channel id
 }
 
 type libcall struct {
diff -Naur /usr/local/go/src/runtime/select.go /usr/local/go-new/go/src/runtime/select.go
--- /usr/local/go/src/runtime/select.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/runtime/select.go	2020-12-08 14:56:51.531740688 -0700
@@ -315,6 +315,8 @@
 			sg.releasetime = -1
 		}
 		sg.c = c
+		sg.cid = c.id //goTrace
+
 		// Construct waiting list in lock order.
 		*nextp = sg
 		nextp = &sg.waitlink
@@ -324,6 +326,7 @@
 			c.recvq.enqueue(sg)
 
 		case caseSend:
+			traceChSend(c.id,sg.eventid, sg.value,4) //goTrace: trace send event, pos=4 --> SELECT: a recv is waiting (send selected)
 			c.sendq.enqueue(sg)
 		}
 	}
@@ -405,6 +408,7 @@
 	}
 
 	if cas.kind == caseRecv {
+		traceChRecv(c.id, sg.eventid, sg.value,5) //goTrace: trace recv event. pos=5 --> SELECT: a sender is waiting (recv selected)
 		recvOK = true
 	}
 
@@ -482,6 +486,7 @@
 
 rclose:
 	// read at end of closed channel
+	traceChRecv(c.id,0,elem2int(cas.elem),6) //goTrace: trace recv event. pos=6 --> SELECT: recv on close (ch_eid=0 --> no matching send) 
 	selunlock(scases, lockorder)
 	recvOK = false
 	if cas.elem != nil {
diff -Naur /usr/local/go/src/runtime/trace.go /usr/local/go-new/go/src/runtime/trace.go
--- /usr/local/go/src/runtime/trace.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/runtime/trace.go	2020-12-08 14:37:52.749261512 -0700
@@ -68,7 +68,17 @@
 	traceEvUserTaskEnd       = 46 // end of a task [timestamp, internal task id, stack]
 	traceEvUserRegion        = 47 // trace.WithRegion [timestamp, internal task id, mode(0:start, 1:end), stack, name string]
 	traceEvUserLog           = 48 // trace.Log [timestamp, internal task id, key string id, stack, value string]
-	traceEvCount             = 49
+	traceEvChSend            = 49 // goTrace: chan send [timestamp, stack, channel id, ch_event id, value, pos]
+        traceEvChRecv            = 50 // goTrace: chan recv [timestamp, stack, channel id, ch_event id, value, pos]
+        traceEvChOp              = 51 // goTrace: chan make/close [timestamp, stack, channel id, value{0:close,1:make}]
+        traceEvWgAdd             = 52 // goTrace: wg add (and inited) [timestamp, stack, wg id, value]
+        traceEvWgWait            = 53 // goTrace: wg wait [timestamp, stack, wg id, pos]
+        traceEvMuLock            = 54 // goTrace: mu lock [timestamp, stack, mu id, pos]
+        traceEvMuUnlock          = 55 // goTrace: mu unlock [timestamp, stack, mu id]
+        traceEvSelect            = 56 // goTrace: select [timestamp, stack, pos]
+        traceEvSched             = 57 // goTrace: sched [timestamp, stack, pos, curg, aux]
+        traceEvCount             = 58
+
 	// Byte is used but only 6 bits are available for event type.
 	// The remaining 2 bits are used to specify the number of arguments.
 	// That means, the max event type value is 63.
@@ -1228,3 +1238,42 @@
 
 	traceReleaseBuffer(pid)
 }
+
+func traceSelect(pos uint64){
+  traceEvent(traceEvSelect, 2, pos)
+}
+
+func traceChSend(cid, eid, val, pos uint64){
+  traceEvent(traceEvChSend, 2, cid, eid, val, pos)
+}
+
+
+func traceChRecv(cid, eid, val, pos uint64){
+  traceEvent(traceEvChRecv, 2, cid, eid, val, pos)
+}
+
+
+func traceChOp(cid, val uint64){
+  traceEvent(traceEvChOp, 2, cid, val)
+}
+
+func TraceWgAdd(wgid ,val uint64){
+  traceEvent(traceEvWgAdd, 2, wgid, val)
+}
+
+func TraceWgWait(wgid, pos uint64){
+  traceEvent(traceEvWgWait, 2, wgid, pos)
+}
+
+func TraceMuLock(muid, pos uint64){
+  traceEvent(traceEvMuLock, 2, muid, pos)
+}
+
+func TraceMuUnlock(muid uint64){
+  traceEvent(traceEvMuUnlock, 2, muid)
+}
+
+
+func traceSched(pos, curg, aux uint64){
+  traceEvent(traceEvSched, 1, pos, curg, aux)
+}
diff -Naur /usr/local/go/src/sync/mutex.go /usr/local/go-new/go/src/sync/mutex.go
--- /usr/local/go/src/sync/mutex.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/sync/mutex.go	2020-12-08 03:38:12.438283715 -0700
@@ -14,6 +14,7 @@
 	"internal/race"
 	"sync/atomic"
 	"unsafe"
+	"runtime"
 )
 
 func throw(string) // provided by runtime
@@ -25,6 +26,8 @@
 type Mutex struct {
 	state int32
 	sema  uint32
+	id    uint64   // goTrace
+	init  bool     // goTrace
 }
 
 // A Locker represents an object that can be locked and unlocked.
@@ -33,6 +36,10 @@
 	Unlock()
 }
 
+var (
+	muID  uint64 = 1 // goTrace
+)
+
 const (
 	mutexLocked = 1 << iota // mutex is locked
 	mutexWoken
@@ -75,10 +82,26 @@
 		if race.Enabled {
 			race.Acquire(unsafe.Pointer(m))
 		}
-		return
+		// goTrace: increment global id and assign to mu if not inited already
+		if !m.init{
+			muID = atomic.AddUint64(&muID,uint64(1))
+			m.id = muID
+			m.init = true
+		} // end goTrace
+		runtime.TraceMuLock(m.id,1) // goTrace: trace m.Lock event. pos=1 --> mutex is free (unlocked)
+ 		return
 	}
+	// goTrace: increment global id and assign to mu if not inited already
+	if !m.init{
+		muID = atomic.AddUint64(&muID,uint64(1))
+		m.id = muID
+		m.init = true
+	} // end goTrace
+	runtime.TraceMuLock(m.id,0) // goTrace: trace m.Lock event. pos=0 --> mutex is locked so BLOCKED
 	// Slow path (outlined so that the fast path can be inlined)
 	m.lockSlow()
+	// now capture the lock event
+	runtime.TraceMuLock(m.id,2) // goTrace: trace m.Lock event. pos=2 --> mutex is woken up(unlocked/UNBLOCKED) now lock
 }
 
 func (m *Mutex) lockSlow() {
@@ -184,6 +207,7 @@
 
 	// Fast path: drop lock bit.
 	new := atomic.AddInt32(&m.state, -mutexLocked)
+	runtime.TraceMuUnlock(m.id) // goTrace: trace m.Unlock event
 	if new != 0 {
 		// Outlined slow path to allow inlining the fast path.
 		// To hide unlockSlow during tracing we skip one extra frame when tracing GoUnblock.
diff -Naur /usr/local/go/src/sync/waitgroup.go /usr/local/go-new/go/src/sync/waitgroup.go
--- /usr/local/go/src/sync/waitgroup.go	2020-12-03 10:32:36.000000000 -0700
+++ /usr/local/go-new/go/src/sync/waitgroup.go	2020-12-08 03:49:45.574696847 -0700
@@ -8,6 +8,7 @@
 	"internal/race"
 	"sync/atomic"
 	"unsafe"
+	"runtime"
 )
 
 // A WaitGroup waits for a collection of goroutines to finish.
@@ -26,8 +27,16 @@
 	// the aligned 8 bytes in them as state, and the other 4 as storage
 	// for the sema.
 	state1 [3]uint32
+
+	id     uint64 // goTrace
+	init   bool   // goTrace
 }
 
+// goTrace - stores unique wg id
+var(
+	wgID uint64 = 1 // goTrace
+)
+
 // state returns pointers to the state and sema fields stored within wg.state1.
 func (wg *WaitGroup) state() (statep *uint64, semap *uint32) {
 	if uintptr(unsafe.Pointer(&wg.state1))%8 == 0 {
@@ -51,6 +60,13 @@
 // new Add calls must happen after all previous Wait calls have returned.
 // See the WaitGroup example.
 func (wg *WaitGroup) Add(delta int) {
+	// goTrace: increment global id and assign to wg if not inited already
+	if !wg.init{
+		wgID = atomic.AddUint64(&wgID,uint64(1))
+		wg.id = wgID
+		wg.init = true
+	} // end goTrace
+
 	statep, semap := wg.state()
 	if race.Enabled {
 		_ = *statep // trigger nil deref early
@@ -77,6 +93,7 @@
 		panic("sync: WaitGroup misuse: Add called concurrently with Wait")
 	}
 	if v > 0 || w == 0 {
+		runtime.TraceWgAdd(wg.id, uint64(delta)) // goTrace: trace wg.Add event
 		return
 	}
 	// This goroutine has set counter to 0 when waiters > 0.
@@ -116,6 +133,7 @@
 				race.Enable()
 				race.Acquire(unsafe.Pointer(wg))
 			}
+			runtime.TraceWgWait(wg.id,1)  // goTrace: trace wg.Wait event. pos=1 -> unblocking wait
 			return
 		}
 		// Increment waiters count.
@@ -127,6 +145,7 @@
 				// otherwise concurrent Waits will race with each other.
 				race.Write(unsafe.Pointer(semap))
 			}
+			runtime.TraceWgWait(wg.id,0)  // goTrace: trace wg.Wait event. pos=0 -> blocked
 			runtime_Semacquire(semap)
 			if *statep != 0 {
 				panic("sync: WaitGroup is reused before previous Wait has returned")
@@ -135,6 +154,7 @@
 				race.Enable()
 				race.Acquire(unsafe.Pointer(wg))
 			}
+			runtime.TraceWgWait(wg.id,2)  // goTrace: trace wg.Wait event. pos=2 -> woken up (unblocked)
 			return
 		}
 	}
